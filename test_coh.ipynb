{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对CoH模块的测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 日期格式转化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'on the 27th day'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from coh.utils import int_to_ordinal\n",
    "int_to_ordinal(663)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LLM筛选文本处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sda', '3', 'e']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from coh.utils import parse_ids_to_list\n",
    "parse_ids_to_list(\"1, 3, 5\", [\"asd\", \"sda\", \"a\", \"3\", \"t\", \"e\", \"w\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* vllm LLM模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-28 23:38:16 [__init__.py:239] Automatically detected platform cuda.\n",
      "WARNING 05-28 23:38:18 [config.py:2972] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 05-28 23:38:25 [config.py:717] This model supports multiple tasks: {'classify', 'reward', 'embed', 'generate', 'score'}. Defaulting to 'generate'.\n",
      "INFO 05-28 23:38:26 [gptq_marlin.py:147] Detected that the model can run with gptq_marlin, however you specified quantization=gptq explicitly, so forcing gptq. Use quantization=gptq_marlin for faster inference\n",
      "INFO 05-28 23:38:26 [gptq_bitblas.py:168] Detected that the model can run with gptq_bitblas, however you specified quantization=gptq explicitly, so forcing gptq. Use quantization=gptq_bitblas for faster inference\n",
      "WARNING 05-28 23:38:26 [config.py:830] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 05-28 23:38:26 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 05-28 23:38:26 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 05-28 23:38:26 [utils.py:2382] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 05-28 23:38:29 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 05-28 23:38:32 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/root/autodl-fs/models/TheBloke/Mixtral-8x7B-Instruct-v0___1-GPTQ/', speculative_config=None, tokenizer='/root/autodl-fs/models/TheBloke/Mixtral-8x7B-Instruct-v0___1-GPTQ/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/root/autodl-fs/models/TheBloke/Mixtral-8x7B-Instruct-v0___1-GPTQ/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 23:38:32,460 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-28 23:38:32 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fd07fc14070>\n",
      "INFO 05-28 23:38:32 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-28 23:38:32 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 05-28 23:38:32 [topk_topp_sampler.py:44] Currently, FlashInfer top-p & top-k sampling sampler is disabled because FlashInfer>=v0.2.3 is not backward compatible. Falling back to the PyTorch-native implementation of top-p & top-k sampling.\n",
      "INFO 05-28 23:38:32 [gpu_model_runner.py:1329] Starting to load model /root/autodl-fs/models/TheBloke/Mixtral-8x7B-Instruct-v0___1-GPTQ/...\n",
      "WARNING 05-28 23:38:32 [utils.py:168] The model class MixtralForCausalLM has not defined `packed_modules_mapping`, this may lead to incorrect mapping of quantized or ignored modules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [01:27<00:00, 87.53s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [01:27<00:00, 87.53s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-28 23:40:02 [loader.py:458] Loading weights took 87.98 seconds\n",
      "INFO 05-28 23:40:03 [gpu_model_runner.py:1347] Model loading took 22.1664 GiB and 89.980749 seconds\n",
      "INFO 05-28 23:40:08 [kv_cache_utils.py:634] GPU KV cache size: 38,048 tokens\n",
      "INFO 05-28 23:40:08 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 1.16x\n",
      "INFO 05-28 23:40:08 [core.py:159] init engine (profile, create kv cache, warmup model) took 5.51 seconds\n",
      "INFO 05-28 23:40:08 [core_client.py:439] Core engine process 0 ready.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<vllm.entrypoints.llm.LLM at 0x7f2420777a30>,\n",
       " {'coh_pick': {'max_tokens': 128,\n",
       "   'top_p': 1.0,\n",
       "   'temperature': 0.0,\n",
       "   'stop': ['</s>', '[/INST]']},\n",
       "  'coh_predict': {'max_tokens': 256,\n",
       "   'top_p': 1.0,\n",
       "   'temperature': 0.0,\n",
       "   'stop': ['</s>', '[/INST]']},\n",
       "  'fliter': {'max_tokens': 16,\n",
       "   'top_p': 1.0,\n",
       "   'temperature': 0.0,\n",
       "   'stop': ['</s>', '[/INST]']}})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import vllm_builder\n",
    "\n",
    "model_name = \"mixtral\"\n",
    "\n",
    "llm, task_params = vllm_builder(model_name)\n",
    "\n",
    "llm, task_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* transformers LLM模型加载测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import torch\n",
    "\n",
    "# model_name = \"./models/TheBloke/Mixtral-8x7B-Instruct-v0___1-GPTQ/\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     model_name, \n",
    "#     trust_remote_code=True,\n",
    "#     )\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# llm = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name, \n",
    "#     trust_remote_code=True,\n",
    "#     device_map=\"auto\",\n",
    "#     torch_dtype=torch.float16\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 数据集加载测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seen dataset proportion: 0.9309428950863213\n",
      "unseen dataset proportion: 0.06905710491367861\n"
     ]
    }
   ],
   "source": [
    "from utils import Data\n",
    "\n",
    "contents = Data(\n",
    "    model_name=model_name,\n",
    "    llm = llm,\n",
    "    param = task_params['fliter'],\n",
    "    \n",
    "    dataset=\"ICEWS14_forecasting\", \n",
    "    add_reverse_relation=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LLM预测文本处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['independent corrupt practices commission', 'education (nigeria)', 'muslim (nigeria)', 'member of the judiciary (nigeria)', 'citizen (nigeria)', 'boko haram']\n"
     ]
    }
   ],
   "source": [
    "from coh.utils import parse_predict_answer\n",
    "\n",
    "str = \"\"\"Possible answers:\n",
    "1. Independent Corrupt Practices Commission\n",
    "2. Education (Nigeria)\n",
    "3. Muslim (Nigeria)\n",
    "4. Member of the Judiciary (Nigeria)\n",
    "5. Citizen (Nigeria)\n",
    "6. Boko Haram\n",
    "7. Government (Nigeria)\n",
    "\"\"\"\n",
    "\n",
    "print(parse_predict_answer(str, contents.entity2id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CoH模块加载测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-05-28 23:52:25] INFO model.py:58: testing LLM...\n",
      "[2025-05-28 23:52:25] ERROR model.py:108: Error testing LLM: 'coh_predict'\n",
      "[2025-05-28 23:52:25] INFO model.py:60: LLM is working!\n"
     ]
    }
   ],
   "source": [
    "from coh import CoH\n",
    "\n",
    "coh = CoH(\n",
    "    num_entities=contents.num_entities,\n",
    "    num_relations=contents.num_relations,\n",
    "    entity2id=contents.entity2id,\n",
    "    id2entity=contents.id2entity,\n",
    "    id2relation=contents.id2relation,\n",
    "    s_his_dict=contents.get_adj_his_format_dict(),\n",
    "    llm=llm,\n",
    "    # tokenizer=tokenizer,\n",
    "    params=task_params.get(\"coh\", {}),\n",
    "    expand_n=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 取10条数据测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([np.int64(0), np.int64(18), np.int64(7536)], np.int64(384))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = contents.test_data\n",
    "test_data = test_data[:10]\n",
    "\n",
    "def extract_data(data):\n",
    "    s, p, o, t = data\n",
    "    return [s, p, t], o\n",
    "\n",
    "q, a = zip(*[extract_data(item) for item in test_data])\n",
    "\n",
    "q[0], a[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 推理过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feb69f50e5b94531b5becf462fd60482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/21084 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = coh(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 样例研究"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "target: Cabinet / Council of Ministers / Advisors (United States)\tConsult\tSergey Viktorovich Lavrov\ton the 314th day\n",
      "\n",
      "John Kerry: tensor(0.3543)\n",
      "\n",
      "Abdullah Abdullah: tensor(0.0110)\n",
      "\n",
      "Ashraf Ghani Ahmadzai: tensor(0.0001)\n",
      "\n",
      "Sergey Viktorovich Lavrov: tensor(0.2891)\n",
      "\n",
      "Algirdas Butkevičius: tensor(0.2315)\n",
      "\n",
      "--------------------------------------------------\n",
      "target: Benjamin Netanyahu\tConsult\tLikud\ton the 314th day\n",
      "\n",
      "China: tensor(0.3543)\n",
      "\n",
      "Barack Obama: tensor(0.4256)\n",
      "\n",
      "Xi Jinping: tensor(0.2891)\n",
      "\n",
      "--------------------------------------------------\n",
      "target: Japan\tEngage in diplomatic cooperation\tChina\ton the 314th day\n",
      "\n",
      "South Korea: tensor(0.4256)\n",
      "\n",
      "China: tensor(0.3543)\n",
      "\n",
      "Other Authorities / Officials (United States): tensor(0.2891)\n",
      "\n",
      "--------------------------------------------------\n",
      "target: Japan\tArrest, detain, or charge with legal action\tCitizen (South Korea)\ton the 314th day\n",
      "\n",
      "--------------------------------------------------\n",
      "target: Japan\tMake an appeal or request\tChina\ton the 314th day\n",
      "\n",
      "China: tensor(0.4256)\n",
      "\n",
      "France: tensor(0.3543)\n",
      "\n",
      "--------------------------------------------------\n",
      "target: Japan\tExpress intent to cooperate\tChina\ton the 314th day\n",
      "\n",
      "South Korea: tensor(0.3543)\n",
      "\n",
      "China: tensor(0.4256)\n",
      "\n",
      "Barack Obama: tensor(0.2315)\n",
      "\n",
      "Other Authorities / Officials (United States): tensor(0.2891)\n",
      "\n",
      "--------------------------------------------------\n",
      "target: North Korea\tReduce relations\tCitizen (North Korea)\ton the 314th day\n",
      "\n",
      "Japan: tensor(0.4256)\n",
      "\n",
      "Citizen (North Korea): tensor(0.0025)\n",
      "\n",
      "South Korea: tensor(0.2315)\n",
      "\n",
      "China: tensor(0.3543)\n",
      "\n",
      "Barack Obama: tensor(0.1419)\n",
      "\n",
      "Xi Jinping: tensor(0.1824)\n",
      "\n",
      "United Arab Emirates: tensor(0.2891)\n",
      "\n",
      "Other Authorities / Officials (United States): tensor(0.1091)\n",
      "\n",
      "Citizen (South Korea): tensor(0.0356)\n",
      "\n",
      "Citizen (United Arab Emirates): tensor(0.0082)\n",
      "\n",
      "Head of Government (South Korea): tensor(0.0061)\n",
      "\n",
      "Head of Government (China): tensor(0.0110)\n",
      "\n",
      "Government Official (Japan): tensor(0.0198)\n",
      "\n",
      "Lawyer/Attorney (South Korea): tensor(0.0033)\n",
      "\n",
      "Military Personnel (South Korea): tensor(0.0045)\n",
      "\n",
      "Jeffrey Fowle: tensor(0.0266)\n",
      "\n",
      "--------------------------------------------------\n",
      "target: North Korea\tDemand\tCitizen (South Korea)\ton the 314th day\n",
      "\n",
      "China: tensor(0.4256)\n",
      "\n",
      "Afghanistan: tensor(0.0004)\n",
      "\n",
      "Iran: tensor(0.0061)\n",
      "\n",
      "Barack Obama: tensor(0.3543)\n",
      "\n",
      "Xi Jinping: tensor(0.2315)\n",
      "\n",
      "Qatar: tensor(0.0018)\n",
      "\n",
      "Thailand: tensor(0.0148)\n",
      "\n",
      "United Arab Emirates: tensor(0.0045)\n",
      "\n",
      "Iraq: tensor(0.0006)\n",
      "\n",
      "South Africa: tensor(0.0356)\n",
      "\n",
      "Malaysia: tensor(0.0198)\n",
      "\n",
      "Philippines: tensor(0.0110)\n",
      "\n",
      "Mexico: tensor(0.0266)\n",
      "\n",
      "Other Authorities / Officials (United States): tensor(0.0832)\n",
      "\n",
      "Sudan: tensor(0.0007)\n",
      "\n",
      "Canada: tensor(0.0474)\n",
      "\n",
      "Colombia: tensor(0.0082)\n",
      "\n",
      "Morocco: tensor(0.0010)\n",
      "\n",
      "Kuwait: tensor(0.0033)\n",
      "\n",
      "Oman: tensor(0.0025)\n",
      "\n",
      "Bahrain: tensor(0.0014)\n",
      "\n",
      "Myanmar: tensor(0.2891)\n",
      "\n",
      "Government Official (Japan): tensor(0.1091)\n",
      "\n",
      "Military Personnel (United States): tensor(0.1419)\n",
      "\n",
      "Jeffrey Fowle: tensor(0.0630)\n",
      "\n",
      "--------------------------------------------------\n",
      "target: Kim Jong-Un\tMake a visit\tNurse (North Korea)\ton the 314th day\n",
      "\n",
      "Japan: tensor(0.3543)\n",
      "\n",
      "North Korea: tensor(0.2891)\n",
      "\n",
      "South Korea: tensor(0.2315)\n",
      "\n",
      "China: tensor(0.4256)\n",
      "\n",
      "Barack Obama: tensor(0.1824)\n",
      "\n",
      "Detainee (United States): tensor(0.1419)\n",
      "\n",
      "--------------------------------------------------\n",
      "target: South Korea\tMake empathetic comment\tCitizen (South Korea)\ton the 314th day\n",
      "\n",
      "Japan: tensor(0.2891)\n",
      "\n",
      "North Korea: tensor(0.4256)\n",
      "\n",
      "China: tensor(0.3543)\n",
      "\n",
      "France: tensor(0.2315)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores.shape\n",
    "scores # (batch_size, num_entities)\n",
    "\n",
    "q_list = coh.format_his_list([s, p, o, t] for [s, p, t], o in zip(q, a))\n",
    "\n",
    "# q_list\n",
    "\n",
    "for score, _q in zip(scores, q_list):\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(\"target: \" + _q + '\\n')\n",
    "    for i in range(len(score)):\n",
    "        if score[i] != 0:\n",
    "            print(contents.id2entity[i] + ': ' + str(score[i]))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 评估测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- coh test ---\n",
      "MRR (%): 30.72\n",
      "Hits@1 (%): 20.00\n",
      "Hits@3 (%): 40.00\n",
      "Hits@10 (%): 40.00\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "from utils import Evaluator\n",
    "\n",
    "eval = Evaluator(\"coh test\")\n",
    "eval.update(score_batch=scores, true_list=a)\n",
    "\n",
    "eval.print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
