{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-23 03:49:42 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "# test vllm\n",
    "\n",
    "from vllm import LLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-23 03:49:47 [config.py:2972] Casting torch.bfloat16 to torch.float16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-23 03:49:55 [config.py:717] This model supports multiple tasks: {'generate', 'embed', 'reward', 'score', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 05-23 03:49:57 [gptq_marlin.py:147] Detected that the model can run with gptq_marlin, however you specified quantization=gptq explicitly, so forcing gptq. Use quantization=gptq_marlin for faster inference\n",
      "INFO 05-23 03:49:57 [gptq_bitblas.py:168] Detected that the model can run with gptq_bitblas, however you specified quantization=gptq explicitly, so forcing gptq. Use quantization=gptq_bitblas for faster inference\n",
      "WARNING 05-23 03:49:57 [config.py:830] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 05-23 03:49:57 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 05-23 03:49:57 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 05-23 03:49:57 [utils.py:2382] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 05-23 03:50:00 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 05-23 03:50:03 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='./models/TheBloke/Mixtral-8x7B-Instruct-v0___1-GPTQ/', speculative_config=None, tokenizer='./models/TheBloke/Mixtral-8x7B-Instruct-v0___1-GPTQ/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=./models/TheBloke/Mixtral-8x7B-Instruct-v0___1-GPTQ/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}\n",
      "WARNING 05-23 03:50:04 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f0eb533a820>\n",
      "INFO 05-23 03:50:04 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-23 03:50:04 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "WARNING 05-23 03:50:04 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 05-23 03:50:04 [gpu_model_runner.py:1329] Starting to load model ./models/TheBloke/Mixtral-8x7B-Instruct-v0___1-GPTQ/...\n",
      "WARNING 05-23 03:50:04 [utils.py:168] The model class MixtralForCausalLM has not defined `packed_modules_mapping`, this may lead to incorrect mapping of quantized or ignored modules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.89s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.89s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-23 03:50:09 [loader.py:458] Loading weights took 3.80 seconds\n",
      "INFO 05-23 03:50:10 [gpu_model_runner.py:1347] Model loading took 22.1664 GiB and 5.911028 seconds\n",
      "INFO 05-23 03:50:15 [kv_cache_utils.py:634] GPU KV cache size: 38,048 tokens\n",
      "INFO 05-23 03:50:15 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 1.16x\n",
      "INFO 05-23 03:50:16 [core.py:159] init engine (profile, create kv cache, warmup model) took 5.56 seconds\n",
      "INFO 05-23 03:50:16 [core_client.py:439] Core engine process 0 ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "model_name = \"/root/autodl-fs/models/TheBloke/Mixtral-8x7B-Instruct-v0___1-GPTQ/\"\n",
    "\n",
    "llm = LLM(model_name, enforce_eager=True, quantization=\"gptq\", dtype=torch.float16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04cb85d233684b5394d78c2754c810bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Prompt: \n",
      "You must be able to correctly predict the {whom} of the given query from a given text consisting\n",
      "of multiple historical events in the form of “{subject} {relation} {object} {time}” and the query in\n",
      "the form of “{subject} {relation} {whom} {time}?” You must output several {object} that you\n",
      "think may be the answer to the given query based on the given historical events. Please list all\n",
      "possible {object} which may be answers to the query. Please assign each answer a serial number\n",
      "to represent its probability of being the correct answer. Note that answers with a high probability\n",
      "of being correct should be listed first.\n",
      "Here are the given historical events:\n",
      "\n",
      "Government_(Nigeria), Engage_in_diplomatic_cooperation with, Independent_Corrupt_Practices_Commission, on the 339th day; Independent_Corrupt_Practices_Commission, Arrest_or_detain_or_charge_with_legal_action to, Citizen_(Nigeria), on the 308th day;\n",
      "Government_(Nigeria), Criticize_or_denounce, Boko_Haram, on the 337th day; \n",
      "Boko_Haram, Use_conventional_military_force to, Citizen_(Nigeria), on the 336th day;\n",
      "Government_(Nigeria), Threaten, Education_(Nigeria), on the 337th day; \n",
      "Education_(Nigeria), Make_statement to, Muslim_(Nigeria), on the 332nd day;\n",
      "Government_(Nigeria), Make_optimistic_comment on, Citizen_(Nigeria), on the 336th day;\n",
      "Citizen_(Nigeria), Make_an_appeal_or_request to, Member_of_the_Judiciary_(Nigeria), on the 331st day;\n",
      "· · · · · ·\n",
      "    \n",
      "Here is the query:\n",
      "\n",
      "Government_(Nigeria), Make_an_appeal_or_request to, whom, on the 340th day?\n",
      "    \n",
      "lease list all possible {object} which may be answers (one per line) without explanations.\n",
      "Note that answers with high probability should be listed first.\n",
      "For example:\n",
      "\"\"\"\n",
      "Possible answers:\n",
      "1. XXX\n",
      "2. XXX\n",
      "3. XXX\n",
      "· · · · · ·\n",
      "\"\"\"\n",
      "Please strictly follow the above demands for output.\n",
      "\n",
      "Generated Text: \n",
      "\n",
      "Possible answers:\n",
      "1. Independent_Corrupt_Practices_Commission\n",
      "2. Education_(Nigeria)\n",
      "3. Muslim_(Nigeria)\n",
      "4. Member_of_the_Judiciary_(Nigeria)\n",
      "5. Citizen_(Nigeria)\n",
      "6. Boko_Haram\n",
      "7. Government_(Nigeria)\n",
      "['Independent_Corrupt_Practices_Commission', 'Education_(Nigeria)', 'Muslim_(Nigeria)', 'Member_of_the_Judiciary_(Nigeria)', 'Citizen_(Nigeria)', 'Boko_Haram']\n",
      "Finish Reason: stop\n"
     ]
    }
   ],
   "source": [
    "from vllm import SamplingParams\n",
    "from coh.utils import parse_predict_answer\n",
    "\n",
    "# Define the sampling parameters\n",
    "sampling_params = SamplingParams(\n",
    "    max_tokens=8000,    # Set the maximum number of tokens to generate\n",
    "    top_p=1.0,          # Set top-p sampling to 1.0 (equivalent to greedy decoding if temperature is 0)\n",
    "    temperature=0.0     # Set temperature to 0.0 for deterministic (greedy) sampling\n",
    ")\n",
    "\n",
    "from coh.prompts import HIS_TO_ANSWER\n",
    "\n",
    "outputs = llm.generate(HIS_TO_ANSWER.format(\n",
    "    histories=\"\"\"\n",
    "Government_(Nigeria), Engage_in_diplomatic_cooperation with, Independent_Corrupt_Practices_Commission, on the 339th day; Independent_Corrupt_Practices_Commission, Arrest_or_detain_or_charge_with_legal_action to, Citizen_(Nigeria), on the 308th day;\n",
    "Government_(Nigeria), Criticize_or_denounce, Boko_Haram, on the 337th day; \n",
    "Boko_Haram, Use_conventional_military_force to, Citizen_(Nigeria), on the 336th day;\n",
    "Government_(Nigeria), Threaten, Education_(Nigeria), on the 337th day; \n",
    "Education_(Nigeria), Make_statement to, Muslim_(Nigeria), on the 332nd day;\n",
    "Government_(Nigeria), Make_optimistic_comment on, Citizen_(Nigeria), on the 336th day;\n",
    "Citizen_(Nigeria), Make_an_appeal_or_request to, Member_of_the_Judiciary_(Nigeria), on the 331st day;\n",
    "· · · · · ·\n",
    "    \"\"\",\n",
    "    query=\"\"\"\n",
    "Government_(Nigeria), Make_an_appeal_or_request to, whom, on the 340th day?\n",
    "    \"\"\"\n",
    "), sampling_params=sampling_params)\n",
    "\n",
    "\n",
    "for output in outputs:\n",
    "    # The original prompt that was sent\n",
    "    original_prompt = output.prompt\n",
    "    print(f\"Original Prompt: {original_prompt}\")\n",
    "\n",
    "    # Access the generated completions\n",
    "    for completion in output.outputs:\n",
    "        generated_text = completion.text\n",
    "        finish_reason = completion.finish_reason\n",
    "        # You can also access completion.token_ids, completion.cumulative_logprob, etc.\n",
    "        print(f\"Generated Text: {generated_text}\")\n",
    "        print(parse_predict_answer(generated_text))\n",
    "        print(f\"Finish Reason: {finish_reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
