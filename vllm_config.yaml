mixtral: 
  llm_params: 
    model: /root/autodl-fs/models/TheBloke/Mixtral-8x7B-Instruct-v0___1-GPTQ/
    enforce_eager: true
    quantization: "gptq"
    dtype: "torch.float16"
    tensor_parallel_size: 1

  task_params:
    coh_pick:
      max_tokens: 128
      top_p: 1.0
      temperature: 0.0
      stop: ["</s>", "[/INST]"]

    coh_predict:
      max_tokens: 256
      top_p: 1.0
      temperature: 0.0
      stop: ["</s>", "[/INST]"]

    fliter: 
      max_tokens: 16
      top_p: 1.0
      temperature: 0.0
      stop: ["</s>", "[/INST]"]