mixtral: 
  llm_params: 
    model: /root/autodl-fs/models/TheBloke/Mixtral-8x7B-Instruct-v0___1-GPTQ/
    enforce_eager: true
    quantization: "gptq"
    dtype: "torch.float16"

  task_params:
    coh:
      max_tokens: 256
      top_p: 1.0
      temperature: 0.0
      stop: ["</s>", "[/INST]"]

    fliter: 
      max_tokens: 16
      top_p: 1.0
      temperature: 0.0
      stop: ["</s>", "[/INST]"]